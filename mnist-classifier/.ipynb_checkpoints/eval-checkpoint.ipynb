{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d33a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라이브러리 추가하기\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5a3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45e35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9250b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = './checkpoint'\n",
    "log_dir = './log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4aff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Setting \n",
    "device = torch.device('gpu' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23aba14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1, padding=0, bias=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1, padding=0, bias=True)\n",
    "        self.drop2 = nn.Dropout2d(p=0.5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=320, out_features=50, bias=True)\n",
    "        self.relu1_fc1 = nn.ReLU()\n",
    "        self.drop1_fc1 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1_fc1(x)\n",
    "        x = self.drop1_fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff93164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(ckpt_dir, net, optim):\n",
    "    ckpt_lst = os.listdir(ckpt_dir)\n",
    "    ckpt_lst.sort()\n",
    "\n",
    "    dict_model = torch.load('./%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
    "\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "\n",
    "    return net, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f0676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minist Dataset Load\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "dataset = datasets.MNIST(download=True, root='./',  train=False, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdda858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = len(loader.dataset)\n",
    "num_batch = np.ceil(num_data / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "718539f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "params = net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f4a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_loss = nn.CrossEntropyLoss().to(device)\n",
    "fn_pred = lambda output: torch.softmax(output, dim=1)\n",
    "fn_acc = lambda pred, label: ((pred.max(dim=1)[1] == label).type(torch.float)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a365d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(params, lr=lr)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e9120fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "net, optim = load(ckpt_dir, net, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be1296a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: BATCH 0001/0157 | LOSS: 0.0230 | ACC 0.9844\n",
      "TEST: BATCH 0002/0157 | LOSS: 0.0133 | ACC 0.9922\n",
      "TEST: BATCH 0003/0157 | LOSS: 0.0135 | ACC 0.9948\n",
      "TEST: BATCH 0004/0157 | LOSS: 0.0106 | ACC 0.9961\n",
      "TEST: BATCH 0005/0157 | LOSS: 0.0147 | ACC 0.9938\n",
      "TEST: BATCH 0006/0157 | LOSS: 0.0173 | ACC 0.9922\n",
      "TEST: BATCH 0007/0157 | LOSS: 0.0216 | ACC 0.9911\n",
      "TEST: BATCH 0008/0157 | LOSS: 0.0359 | ACC 0.9883\n",
      "TEST: BATCH 0009/0157 | LOSS: 0.0325 | ACC 0.9896\n",
      "TEST: BATCH 0010/0157 | LOSS: 0.0346 | ACC 0.9891\n",
      "TEST: BATCH 0011/0157 | LOSS: 0.0343 | ACC 0.9886\n",
      "TEST: BATCH 0012/0157 | LOSS: 0.0373 | ACC 0.9883\n",
      "TEST: BATCH 0013/0157 | LOSS: 0.0346 | ACC 0.9892\n",
      "TEST: BATCH 0014/0157 | LOSS: 0.0338 | ACC 0.9888\n",
      "TEST: BATCH 0015/0157 | LOSS: 0.0392 | ACC 0.9875\n",
      "TEST: BATCH 0016/0157 | LOSS: 0.0434 | ACC 0.9873\n",
      "TEST: BATCH 0017/0157 | LOSS: 0.0434 | ACC 0.9871\n",
      "TEST: BATCH 0018/0157 | LOSS: 0.0422 | ACC 0.9878\n",
      "TEST: BATCH 0019/0157 | LOSS: 0.0413 | ACC 0.9885\n",
      "TEST: BATCH 0020/0157 | LOSS: 0.0464 | ACC 0.9875\n",
      "TEST: BATCH 0021/0157 | LOSS: 0.0470 | ACC 0.9866\n",
      "TEST: BATCH 0022/0157 | LOSS: 0.0463 | ACC 0.9865\n",
      "TEST: BATCH 0023/0157 | LOSS: 0.0474 | ACC 0.9864\n",
      "TEST: BATCH 0024/0157 | LOSS: 0.0484 | ACC 0.9857\n",
      "TEST: BATCH 0025/0157 | LOSS: 0.0469 | ACC 0.9862\n",
      "TEST: BATCH 0026/0157 | LOSS: 0.0466 | ACC 0.9862\n",
      "TEST: BATCH 0027/0157 | LOSS: 0.0492 | ACC 0.9855\n",
      "TEST: BATCH 0028/0157 | LOSS: 0.0494 | ACC 0.9849\n",
      "TEST: BATCH 0029/0157 | LOSS: 0.0477 | ACC 0.9855\n",
      "TEST: BATCH 0030/0157 | LOSS: 0.0483 | ACC 0.9849\n",
      "TEST: BATCH 0031/0157 | LOSS: 0.0470 | ACC 0.9854\n",
      "TEST: BATCH 0032/0157 | LOSS: 0.0489 | ACC 0.9854\n",
      "TEST: BATCH 0033/0157 | LOSS: 0.0489 | ACC 0.9844\n",
      "TEST: BATCH 0034/0157 | LOSS: 0.0509 | ACC 0.9839\n",
      "TEST: BATCH 0035/0157 | LOSS: 0.0505 | ACC 0.9839\n",
      "TEST: BATCH 0036/0157 | LOSS: 0.0503 | ACC 0.9839\n",
      "TEST: BATCH 0037/0157 | LOSS: 0.0510 | ACC 0.9840\n",
      "TEST: BATCH 0038/0157 | LOSS: 0.0506 | ACC 0.9840\n",
      "TEST: BATCH 0039/0157 | LOSS: 0.0512 | ACC 0.9836\n",
      "TEST: BATCH 0040/0157 | LOSS: 0.0500 | ACC 0.9840\n",
      "TEST: BATCH 0041/0157 | LOSS: 0.0525 | ACC 0.9840\n",
      "TEST: BATCH 0042/0157 | LOSS: 0.0538 | ACC 0.9840\n",
      "TEST: BATCH 0043/0157 | LOSS: 0.0530 | ACC 0.9840\n",
      "TEST: BATCH 0044/0157 | LOSS: 0.0523 | ACC 0.9844\n",
      "TEST: BATCH 0045/0157 | LOSS: 0.0512 | ACC 0.9847\n",
      "TEST: BATCH 0046/0157 | LOSS: 0.0523 | ACC 0.9844\n",
      "TEST: BATCH 0047/0157 | LOSS: 0.0522 | ACC 0.9844\n",
      "TEST: BATCH 0048/0157 | LOSS: 0.0518 | ACC 0.9840\n",
      "TEST: BATCH 0049/0157 | LOSS: 0.0510 | ACC 0.9844\n",
      "TEST: BATCH 0050/0157 | LOSS: 0.0501 | ACC 0.9847\n",
      "TEST: BATCH 0051/0157 | LOSS: 0.0492 | ACC 0.9850\n",
      "TEST: BATCH 0052/0157 | LOSS: 0.0487 | ACC 0.9853\n",
      "TEST: BATCH 0053/0157 | LOSS: 0.0484 | ACC 0.9853\n",
      "TEST: BATCH 0054/0157 | LOSS: 0.0483 | ACC 0.9852\n",
      "TEST: BATCH 0055/0157 | LOSS: 0.0479 | ACC 0.9852\n",
      "TEST: BATCH 0056/0157 | LOSS: 0.0500 | ACC 0.9849\n",
      "TEST: BATCH 0057/0157 | LOSS: 0.0497 | ACC 0.9852\n",
      "TEST: BATCH 0058/0157 | LOSS: 0.0489 | ACC 0.9855\n",
      "TEST: BATCH 0059/0157 | LOSS: 0.0500 | ACC 0.9854\n",
      "TEST: BATCH 0060/0157 | LOSS: 0.0500 | ACC 0.9852\n",
      "TEST: BATCH 0061/0157 | LOSS: 0.0496 | ACC 0.9851\n",
      "TEST: BATCH 0062/0157 | LOSS: 0.0493 | ACC 0.9851\n",
      "TEST: BATCH 0063/0157 | LOSS: 0.0491 | ACC 0.9851\n",
      "TEST: BATCH 0064/0157 | LOSS: 0.0490 | ACC 0.9851\n",
      "TEST: BATCH 0065/0157 | LOSS: 0.0483 | ACC 0.9853\n",
      "TEST: BATCH 0066/0157 | LOSS: 0.0489 | ACC 0.9846\n",
      "TEST: BATCH 0067/0157 | LOSS: 0.0494 | ACC 0.9844\n",
      "TEST: BATCH 0068/0157 | LOSS: 0.0489 | ACC 0.9846\n",
      "TEST: BATCH 0069/0157 | LOSS: 0.0484 | ACC 0.9848\n",
      "TEST: BATCH 0070/0157 | LOSS: 0.0479 | ACC 0.9850\n",
      "TEST: BATCH 0071/0157 | LOSS: 0.0477 | ACC 0.9853\n",
      "TEST: BATCH 0072/0157 | LOSS: 0.0473 | ACC 0.9855\n",
      "TEST: BATCH 0073/0157 | LOSS: 0.0479 | ACC 0.9854\n",
      "TEST: BATCH 0074/0157 | LOSS: 0.0477 | ACC 0.9856\n",
      "TEST: BATCH 0075/0157 | LOSS: 0.0479 | ACC 0.9854\n",
      "TEST: BATCH 0076/0157 | LOSS: 0.0487 | ACC 0.9854\n",
      "TEST: BATCH 0077/0157 | LOSS: 0.0485 | ACC 0.9854\n",
      "TEST: BATCH 0078/0157 | LOSS: 0.0485 | ACC 0.9852\n",
      "TEST: BATCH 0079/0157 | LOSS: 0.0479 | ACC 0.9854\n",
      "TEST: BATCH 0080/0157 | LOSS: 0.0474 | ACC 0.9855\n",
      "TEST: BATCH 0081/0157 | LOSS: 0.0470 | ACC 0.9855\n",
      "TEST: BATCH 0082/0157 | LOSS: 0.0465 | ACC 0.9857\n",
      "TEST: BATCH 0083/0157 | LOSS: 0.0460 | ACC 0.9859\n",
      "TEST: BATCH 0084/0157 | LOSS: 0.0454 | ACC 0.9860\n",
      "TEST: BATCH 0085/0157 | LOSS: 0.0449 | ACC 0.9862\n",
      "TEST: BATCH 0086/0157 | LOSS: 0.0444 | ACC 0.9864\n",
      "TEST: BATCH 0087/0157 | LOSS: 0.0439 | ACC 0.9865\n",
      "TEST: BATCH 0088/0157 | LOSS: 0.0436 | ACC 0.9867\n",
      "TEST: BATCH 0089/0157 | LOSS: 0.0431 | ACC 0.9868\n",
      "TEST: BATCH 0090/0157 | LOSS: 0.0428 | ACC 0.9870\n",
      "TEST: BATCH 0091/0157 | LOSS: 0.0424 | ACC 0.9871\n",
      "TEST: BATCH 0092/0157 | LOSS: 0.0420 | ACC 0.9873\n",
      "TEST: BATCH 0093/0157 | LOSS: 0.0421 | ACC 0.9871\n",
      "TEST: BATCH 0094/0157 | LOSS: 0.0426 | ACC 0.9869\n",
      "TEST: BATCH 0095/0157 | LOSS: 0.0422 | ACC 0.9870\n",
      "TEST: BATCH 0096/0157 | LOSS: 0.0424 | ACC 0.9870\n",
      "TEST: BATCH 0097/0157 | LOSS: 0.0428 | ACC 0.9868\n",
      "TEST: BATCH 0098/0157 | LOSS: 0.0423 | ACC 0.9869\n",
      "TEST: BATCH 0099/0157 | LOSS: 0.0419 | ACC 0.9871\n",
      "TEST: BATCH 0100/0157 | LOSS: 0.0415 | ACC 0.9872\n",
      "TEST: BATCH 0101/0157 | LOSS: 0.0411 | ACC 0.9873\n",
      "TEST: BATCH 0102/0157 | LOSS: 0.0415 | ACC 0.9873\n",
      "TEST: BATCH 0103/0157 | LOSS: 0.0425 | ACC 0.9868\n",
      "TEST: BATCH 0104/0157 | LOSS: 0.0446 | ACC 0.9866\n",
      "TEST: BATCH 0105/0157 | LOSS: 0.0441 | ACC 0.9868\n",
      "TEST: BATCH 0106/0157 | LOSS: 0.0442 | ACC 0.9866\n",
      "TEST: BATCH 0107/0157 | LOSS: 0.0438 | ACC 0.9867\n",
      "TEST: BATCH 0108/0157 | LOSS: 0.0434 | ACC 0.9868\n",
      "TEST: BATCH 0109/0157 | LOSS: 0.0430 | ACC 0.9870\n",
      "TEST: BATCH 0110/0157 | LOSS: 0.0426 | ACC 0.9871\n",
      "TEST: BATCH 0111/0157 | LOSS: 0.0423 | ACC 0.9872\n",
      "TEST: BATCH 0112/0157 | LOSS: 0.0420 | ACC 0.9873\n",
      "TEST: BATCH 0113/0157 | LOSS: 0.0416 | ACC 0.9874\n",
      "TEST: BATCH 0114/0157 | LOSS: 0.0413 | ACC 0.9875\n",
      "TEST: BATCH 0115/0157 | LOSS: 0.0409 | ACC 0.9876\n",
      "TEST: BATCH 0116/0157 | LOSS: 0.0406 | ACC 0.9877\n",
      "TEST: BATCH 0117/0157 | LOSS: 0.0406 | ACC 0.9877\n",
      "TEST: BATCH 0118/0157 | LOSS: 0.0403 | ACC 0.9878\n",
      "TEST: BATCH 0119/0157 | LOSS: 0.0399 | ACC 0.9879\n",
      "TEST: BATCH 0120/0157 | LOSS: 0.0396 | ACC 0.9880\n",
      "TEST: BATCH 0121/0157 | LOSS: 0.0393 | ACC 0.9881\n",
      "TEST: BATCH 0122/0157 | LOSS: 0.0390 | ACC 0.9882\n",
      "TEST: BATCH 0123/0157 | LOSS: 0.0389 | ACC 0.9882\n",
      "TEST: BATCH 0124/0157 | LOSS: 0.0387 | ACC 0.9883\n",
      "TEST: BATCH 0125/0157 | LOSS: 0.0384 | ACC 0.9884\n",
      "TEST: BATCH 0126/0157 | LOSS: 0.0382 | ACC 0.9885\n",
      "TEST: BATCH 0127/0157 | LOSS: 0.0381 | ACC 0.9884\n",
      "TEST: BATCH 0128/0157 | LOSS: 0.0378 | ACC 0.9885\n",
      "TEST: BATCH 0129/0157 | LOSS: 0.0377 | ACC 0.9886\n",
      "TEST: BATCH 0130/0157 | LOSS: 0.0376 | ACC 0.9887\n",
      "TEST: BATCH 0131/0157 | LOSS: 0.0374 | ACC 0.9888\n",
      "TEST: BATCH 0132/0157 | LOSS: 0.0372 | ACC 0.9888\n",
      "TEST: BATCH 0133/0157 | LOSS: 0.0369 | ACC 0.9888\n",
      "TEST: BATCH 0134/0157 | LOSS: 0.0368 | ACC 0.9888\n",
      "TEST: BATCH 0135/0157 | LOSS: 0.0365 | ACC 0.9889\n",
      "TEST: BATCH 0136/0157 | LOSS: 0.0362 | ACC 0.9890\n",
      "TEST: BATCH 0137/0157 | LOSS: 0.0360 | ACC 0.9891\n",
      "TEST: BATCH 0138/0157 | LOSS: 0.0357 | ACC 0.9891\n",
      "TEST: BATCH 0139/0157 | LOSS: 0.0355 | ACC 0.9892\n",
      "TEST: BATCH 0140/0157 | LOSS: 0.0352 | ACC 0.9893\n",
      "TEST: BATCH 0141/0157 | LOSS: 0.0359 | ACC 0.9890\n",
      "TEST: BATCH 0142/0157 | LOSS: 0.0358 | ACC 0.9890\n",
      "TEST: BATCH 0143/0157 | LOSS: 0.0356 | ACC 0.9891\n",
      "TEST: BATCH 0144/0157 | LOSS: 0.0353 | ACC 0.9891\n",
      "TEST: BATCH 0145/0157 | LOSS: 0.0351 | ACC 0.9892\n",
      "TEST: BATCH 0146/0157 | LOSS: 0.0349 | ACC 0.9893\n",
      "TEST: BATCH 0147/0157 | LOSS: 0.0346 | ACC 0.9894\n",
      "TEST: BATCH 0148/0157 | LOSS: 0.0344 | ACC 0.9894\n",
      "TEST: BATCH 0149/0157 | LOSS: 0.0342 | ACC 0.9895\n",
      "TEST: BATCH 0150/0157 | LOSS: 0.0340 | ACC 0.9896\n",
      "TEST: BATCH 0151/0157 | LOSS: 0.0344 | ACC 0.9894\n",
      "TEST: BATCH 0152/0157 | LOSS: 0.0347 | ACC 0.9894\n",
      "TEST: BATCH 0153/0157 | LOSS: 0.0352 | ACC 0.9894\n",
      "TEST: BATCH 0154/0157 | LOSS: 0.0351 | ACC 0.9893\n",
      "TEST: BATCH 0155/0157 | LOSS: 0.0351 | ACC 0.9893\n",
      "TEST: BATCH 0156/0157 | LOSS: 0.0351 | ACC 0.9894\n",
      "TEST: BATCH 0157/0157 | LOSS: 0.0349 | ACC 0.9895\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    \n",
    "    loss_arr = []\n",
    "    acc_arr = []\n",
    "    \n",
    "    for batch, (input_d, label) in enumerate(loader, 1):\n",
    "        input_d = input_d.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        output = net(input_d)\n",
    "        pred = fn_pred(output)\n",
    "                \n",
    "        loss = fn_loss(output, label)\n",
    "        acc = fn_acc(pred, label)\n",
    "        \n",
    "        loss_arr.append(loss.item())\n",
    "        acc_arr.append(acc.item())\n",
    "        \n",
    "        print('TEST: BATCH %04d/%04d | LOSS: %.4f | ACC %.4f' %\n",
    "              (batch, num_batch, np.mean(loss_arr), np.mean(acc_arr)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc1d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-tutorial",
   "language": "python",
   "name": "pytorch-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
